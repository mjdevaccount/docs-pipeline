#!/usr/bin/env python3
"""
Professional Markdown to PDF/DOCX/HTML Converter
================================================

Thin wrapper around pipeline module for backward compatibility.

REFACTORED: Complete pipeline-based architecture
- ~75% LOC reduction (1,200 â†’ 300 lines)
- All complex logic moved to tested, modular pipeline
- Maintains 100% backward compatibility

For new code, use pipeline directly:
    from pipeline import process_document, OutputFormat
    success = process_document('doc.md', 'doc.pdf', OutputFormat.PDF, 
                              renderer='playwright', generate_cover=True)

Architecture modules in use:
- external_tools: PandocExecutor, KatexCLI, MermaidCLI
- diagram_rendering: DiagramOrchestrator with pluggable renderers
- metadata: DocumentMetadata extraction, validation, merging
- renderers: RendererFactory with Strategy pattern (WeasyPrint/Playwright)
- pipeline: Pipeline orchestrator with composable steps
"""
import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings

# Suppress RuntimeWarning about module import order when running as -m tools.pdf.convert_final
# This warning is emitted by Python's runpy module before this code executes.
# It occurs because __init__.py may trigger imports that load this module into sys.modules
# before runpy executes it. This is harmless - the module works correctly.
# 
# To suppress: python -W ignore::RuntimeWarning -m tools.pdf.convert_final ...
# Or set: PYTHONWARNINGS=ignore::RuntimeWarning
warnings.filterwarnings('ignore', message='.*found in sys.modules after import of package.*', 
                        category=RuntimeWarning)

# Add module path for imports
sys.path.insert(0, str(Path(__file__).parent))

# Import pipeline module (required)
from pipeline import (
    process_document,
    create_pdf_pipeline,
    create_docx_pipeline,
    create_html_pipeline,
    PipelineContext,
    OutputFormat,
    PipelineConfig
)


def markdown_to_pdf(
    md_file: str,
    output_pdf: str,
    logo_path: Optional[str] = None,
    css_file: Optional[str] = None,
    cache_dir: Optional[str] = None,
    use_cache: bool = True,
    theme_config: Optional[str] = None,
    highlight_style: Optional[str] = None,
    crossref_config: Optional[str] = None,
    glossary_file: Optional[str] = None,
    renderer: str = 'weasyprint',
    generate_toc: bool = False,
    generate_cover: bool = False,
    watermark: Optional[str] = None,
    verbose: bool = False,
    profile: Optional[str] = None,
    custom_metadata: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Convert Markdown to PDF using pipeline architecture.
    
    This is a backward-compatible wrapper. For new code, use:
        from pipeline import process_document, OutputFormat
        success = process_document('doc.md', 'doc.pdf', OutputFormat.PDF, **kwargs)
    
    Args:
        md_file: Path to input Markdown file
        output_pdf: Path to output PDF file
        logo_path: Optional logo image path
        css_file: Optional CSS file path
        cache_dir: Diagram cache directory
        use_cache: Enable diagram caching
        theme_config: Mermaid theme config JSON
        highlight_style: Code highlighting style
        crossref_config: Pandoc-crossref config
        glossary_file: Glossary YAML file
        renderer: PDF renderer ('weasyprint' or 'playwright')
        generate_toc: Generate table of contents
        generate_cover: Generate cover page
        watermark: Watermark text
        verbose: Verbose output
        profile: Profile name (e.g., 'tech-whitepaper')
        custom_metadata: Metadata overrides
    
    Returns:
        True if conversion succeeded, False otherwise
    """
    # Build config dictionary for pipeline
    config = {
        'logo_path': logo_path,
        'css_file': css_file,
        'cache_dir': cache_dir,
        'use_cache': use_cache,
        'theme_config': theme_config,
        'highlight_style': highlight_style or 'pygments',
        'crossref_config': crossref_config,
        'glossary_file': glossary_file,
        'renderer': renderer,
        'generate_toc': generate_toc,
        'generate_cover': generate_cover,
        'watermark': watermark,
        'verbose': verbose,
        'profile': profile,  # CRITICAL: Include profile so CSS is loaded
        'custom_metadata': custom_metadata or {}
    }
    
    # Remove None values
    config = {k: v for k, v in config.items() if v is not None}
    
    if verbose:
        print(f"\nConverting {md_file} to PDF...")
        print(f"  Renderer: {renderer}")
        if generate_cover:
            print(f"  Cover page: enabled")
        if generate_toc:
            print(f"  Table of contents: enabled")
    
    try:
        success = process_document(
            input_file=md_file,
            output_file=output_pdf,
            output_format=OutputFormat.PDF,
            **config
        )
        
        if success and verbose:
            print(f"[OK] Created: {output_pdf}")
        
        return success
        
    except Exception as e:
        if verbose:
            print(f"[ERROR] PDF conversion failed: {e}")
        return False


def markdown_to_docx(
    md_file: str,
    output_docx: str,
    logo_path: Optional[str] = None,
    reference_docx: Optional[str] = None,
    cache_dir: Optional[str] = None,
    use_cache: bool = True,
    theme_config: Optional[str] = None,
    highlight_style: Optional[str] = None,
    crossref_config: Optional[str] = None,
    glossary_file: Optional[str] = None
) -> bool:
    """
    Convert Markdown to DOCX using pipeline architecture.
    
    This is a backward-compatible wrapper. For new code, use:
        from pipeline import process_document, OutputFormat
        success = process_document('doc.md', 'doc.docx', OutputFormat.DOCX, **kwargs)
    
    Args:
        md_file: Path to input Markdown file
        output_docx: Path to output DOCX file
        logo_path: Optional logo (not used in DOCX)
        reference_docx: Reference DOCX template
        cache_dir: Diagram cache directory
        use_cache: Enable diagram caching
        theme_config: Mermaid theme config JSON
        highlight_style: Code highlighting style
        crossref_config: Pandoc-crossref config
        glossary_file: Glossary YAML file
    
    Returns:
        True if conversion succeeded, False otherwise
    """
    config = {
        'reference_docx': reference_docx,
        'cache_dir': cache_dir,
        'use_cache': use_cache,
        'theme_config': theme_config,
        'highlight_style': highlight_style or 'pygments',
        'crossref_config': crossref_config,
        'glossary_file': glossary_file,
        'also_png': True,  # DOCX needs PNG diagrams
        'verbose': True
    }
    
    # Remove None values
    config = {k: v for k, v in config.items() if v is not None}
    
    print(f"\nConverting {md_file} to DOCX...")
    
    try:
        success = process_document(
            input_file=md_file,
            output_file=output_docx,
            output_format=OutputFormat.DOCX,
            **config
        )
        
        if success:
            print(f"[OK] Created: {output_docx}")
        
        return success
        
    except Exception as e:
        print(f"[ERROR] DOCX conversion failed: {e}")
        return False


def markdown_to_html(
    md_file: str,
    output_html: str,
    cache_dir: Optional[str] = None,
    use_cache: bool = True,
    theme_config: Optional[str] = None,
    highlight_style: Optional[str] = None,
    crossref_config: Optional[str] = None,
    glossary_file: Optional[str] = None,
    css_file: Optional[str] = None
) -> bool:
    """
    Convert Markdown to HTML using pipeline architecture.
    
    This is a backward-compatible wrapper. For new code, use:
        from pipeline import process_document, OutputFormat
        success = process_document('doc.md', 'doc.html', OutputFormat.HTML, **kwargs)
    
    Args:
        md_file: Path to input Markdown file
        output_html: Path to output HTML file
        cache_dir: Diagram cache directory
        use_cache: Enable diagram caching
        theme_config: Mermaid theme config JSON
        highlight_style: Code highlighting style
        crossref_config: Pandoc-crossref config
        glossary_file: Glossary YAML file
        css_file: Custom CSS file
    
    Returns:
        True if conversion succeeded, False otherwise
    """
    config = {
        'cache_dir': cache_dir,
        'use_cache': use_cache,
        'theme_config': theme_config,
        'highlight_style': highlight_style or 'pygments',
        'crossref_config': crossref_config,
        'glossary_file': glossary_file,
        'css_file': css_file,
        'verbose': True
    }
    
    # Remove None values
    config = {k: v for k, v in config.items() if v is not None}
    
    print(f"\nConverting {md_file} to HTML...")
    
    try:
        success = process_document(
            input_file=md_file,
            output_file=output_html,
            output_format=OutputFormat.HTML,
            **config
        )
        
        if success:
            print(f"[OK] Created: {output_html}")
        
        return success
        
    except Exception as e:
        print(f"[ERROR] HTML conversion failed: {e}")
        return False


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def get_cache_dir(cache_location: Optional[str] = None) -> Path:
    """
    Get or create cache directory for diagrams.
    
    DEPRECATED: Use DiagramCache from diagram_rendering module instead.
        from diagram_rendering import DiagramCache
        cache = DiagramCache(cache_location)
    """
    if cache_location:
        cache_dir = Path(cache_location)
    else:
        cache_dir = Path(__file__).parent / 'output' / 'pdf-diagrams'
    
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def extract_metadata(md_content: str) -> tuple:
    """
    Extract YAML frontmatter from Markdown.
    
    DEPRECATED: Use MetadataExtractor from metadata module instead.
        from metadata import MetadataExtractor
        extractor = MetadataExtractor()
        metadata, content = extractor.extract_from_string(md_content)
    """
    import yaml
    
    if md_content.startswith('---'):
        parts = md_content.split('---', 2)
        if len(parts) >= 3:
            try:
                metadata = yaml.safe_load(parts[1])
                content = parts[2].strip()
                return metadata if metadata else {}, content
            except:
                pass
    return {}, md_content


def expand_glossary(md_content: str, glossary_file: Optional[str] = None) -> str:
    """
    Expand glossary terms and acronyms in markdown content.
    
    DEPRECATED: Use GlossaryExpansionStep from pipeline.steps instead.
    """
    if not glossary_file or not Path(glossary_file).exists():
        return md_content
    
    try:
        import yaml
        import re
        
        with open(glossary_file, 'r', encoding='utf-8') as f:
            glossary_data = yaml.safe_load(f)
        
        if not glossary_data:
            return md_content
        
        # Expand acronyms
        acronyms = glossary_data.get('acronyms', {})
        for acronym, expansion in acronyms.items():
            pattern = r'\b' + re.escape(acronym) + r'\b'
            replacement = f'{expansion} ({acronym})'
            md_content = re.sub(pattern, replacement, md_content, count=1)
        
        # Add glossary appendix
        terms = glossary_data.get('terms', {})
        if terms:
            glossary_appendix = '\n\n## Glossary\n\n'
            for term, definition in sorted(terms.items()):
                glossary_appendix += f'**{term}**: {definition}\n\n'
            md_content += glossary_appendix
        
        return md_content
    except Exception as e:
        print(f"    ! Warning: Glossary expansion failed: {e}")
        return md_content


def batch_convert(
    input_files: List[str],
    output_format: str = 'pdf',
    **kwargs
) -> Dict[str, bool]:
    """
    Batch convert multiple Markdown files.
    
    Args:
        input_files: List of input Markdown file paths
        output_format: Output format ('pdf', 'docx', 'html')
        **kwargs: Additional arguments passed to converter
    
    Returns:
        Dictionary mapping input files to success status
    """
    results = {}
    
    # Determine converter function
    format_map = {
        'pdf': markdown_to_pdf,
        'docx': markdown_to_docx,
        'html': markdown_to_html
    }
    
    converter = format_map.get(output_format.lower())
    if not converter:
        raise ValueError(f"Unsupported output format: {output_format}")
    
    # Process each file
    for input_file in input_files:
        input_path = Path(input_file)
        if not input_path.exists():
            print(f"[ERROR] File not found: {input_file}")
            results[input_file] = False
            continue
        
        # Determine output file
        output_file = str(input_path.with_suffix(f'.{output_format.lower()}'))
        
        print(f"\n{'='*70}")
        print(f"Converting: {input_file}")
        print(f"Output: {output_file}")
        print(f"{'='*70}")
        
        try:
            success = converter(input_file, output_file, **kwargs)
            results[input_file] = success
            
            if success:
                print(f"[OK] Successfully converted {input_file}")
            else:
                print(f"[FAIL] Failed to convert {input_file}")
        except Exception as e:
            print(f"[ERROR] Exception during conversion: {e}")
            results[input_file] = False
    
    # Summary
    print(f"\n{'='*70}")
    print("BATCH CONVERSION SUMMARY")
    print(f"{'='*70}")
    total = len(results)
    succeeded = sum(1 for success in results.values() if success)
    failed = total - succeeded
    print(f"Total: {total} | Succeeded: {succeeded} | Failed: {failed}")
    print(f"{'='*70}\n")
    
    return results


# =============================================================================
# CLI HELPERS
# =============================================================================

def check_dependencies() -> bool:
    """Check if all required dependencies are available."""
    import shutil
    
    errors = []
    warnings = []
    
    # Check Pandoc
    pandoc = shutil.which('pandoc')
    if not pandoc:
        # Check common Windows locations
        windows_paths = [
            r'C:\Program Files\Pandoc\pandoc.exe',
            r'C:\Program Files (x86)\Pandoc\pandoc.exe',
        ]
        for path in windows_paths:
            if Path(path).exists():
                pandoc = path
                break
    
    if not pandoc:
        errors.append("Pandoc not found. Install from https://pandoc.org/installing.html")
    else:
        print(f"[OK] Pandoc found: {pandoc}")
    
    # Check Mermaid-CLI
    mmdc = shutil.which('mmdc') or shutil.which('mmdc.cmd')
    if not mmdc:
        warnings.append("Mermaid-CLI not found. Diagrams will not render.")
        warnings.append("  Install: npm install -g @mermaid-js/mermaid-cli")
    else:
        print(f"[OK] Mermaid-CLI found: {mmdc}")
    
    # Check Python packages
    try:
        import weasyprint
        print(f"[OK] WeasyPrint {weasyprint.__version__}")
    except ImportError:
        errors.append("WeasyPrint not installed. Run: pip install weasyprint")
    
    try:
        import yaml
        print(f"[OK] PyYAML installed")
    except ImportError:
        errors.append("PyYAML not installed. Run: pip install pyyaml")
    
    try:
        from playwright.sync_api import sync_playwright
        print(f"[OK] Playwright installed")
    except ImportError:
        warnings.append("Playwright not installed. Playwright renderer unavailable.")
    
    if errors:
        print(f"\n[ERROR] Missing required dependencies:")
        for error in errors:
            print(f"  - {error}")
        return False
    
    if warnings:
        print(f"\n[WARN] Optional dependencies missing:")
        for warning in warnings:
            print(f"  - {warning}")
    
    return True


def validate_markdown(md_file: str, verbose: bool = False) -> tuple:
    """
    Validate Markdown file and YAML frontmatter.
    
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    errors = []
    warnings = []
    
    try:
        md_path = Path(md_file)
        if not md_path.exists():
            return False, [f"File not found: {md_file}"]
        
        md_content = md_path.read_text(encoding='utf-8')
        
        # Validate YAML frontmatter
        if md_content.startswith('---'):
            try:
                metadata, _ = extract_metadata(md_content)
                
                if not metadata.get('title'):
                    warnings.append("No 'title' field in frontmatter")
                
                # Check for recommended fields
                for field in ['author', 'date', 'version']:
                    if field not in metadata:
                        warnings.append(f"Missing recommended field: '{field}'")
                        
            except Exception as e:
                errors.append(f"Invalid YAML frontmatter: {e}")
        else:
            warnings.append("No YAML frontmatter found (optional but recommended)")
        
        # Basic Markdown validation
        if not md_content.strip():
            errors.append("Markdown file is empty")
        
        # Check for mismatched code blocks
        code_blocks = md_content.count('```')
        if code_blocks % 2 != 0:
            warnings.append("Possible mismatched code block delimiters")
        
        return len(errors) == 0, errors + warnings
        
    except Exception as e:
        return False, [f"Validation error: {e}"]


def load_config(config_file: str) -> dict:
    """Load configuration from JSON file."""
    import json
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def resolve_output_path(output_file: str, output_dir: str = None) -> str:
    """Resolve output path, applying output_dir if specified."""
    output_path = Path(output_file)
    
    if output_path.is_absolute():
        target = output_path
    elif output_dir:
        target = Path(output_dir) / output_path.name
    else:
        # Default to output/ in project root
        project_root = Path(__file__).parent.parent.parent
        target = project_root / "output" / output_path.name
    
    target.parent.mkdir(parents=True, exist_ok=True)
    return str(target)


def parallel_batch_convert(
    file_tasks: list,
    threads: int,
    verbose: bool = False
) -> dict:
    """
    Parallel batch conversion using ThreadPoolExecutor.
    
    Args:
        file_tasks: List of (input_file, output_file, format, kwargs) tuples
        threads: Number of parallel threads
        verbose: Verbose output
    
    Returns:
        Dictionary mapping input files to success status
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    results = {}
    
    # Try to use tqdm for progress bar
    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False
    
    def convert_task(task):
        input_file, output_file, output_format, kwargs = task
        try:
            format_map = {
                'pdf': markdown_to_pdf,
                'docx': markdown_to_docx,
                'html': markdown_to_html
            }
            converter = format_map[output_format]
            success = converter(input_file, output_file, **kwargs)
            return input_file, output_file, success, None
        except Exception as e:
            return input_file, output_file, False, str(e)
    
    print(f"\n[INFO] Processing {len(file_tasks)} files with {threads} threads...")
    
    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = {executor.submit(convert_task, task): task for task in file_tasks}
        
        if use_tqdm:
            iterator = tqdm(as_completed(futures), total=len(futures), desc="Converting")
        else:
            iterator = as_completed(futures)
        
        for future in iterator:
            input_file, output_file, success, error = future.result()
            results[input_file] = success
            if success:
                if not use_tqdm:
                    print(f"[OK] Generated: {output_file}")
            else:
                print(f"\n[ERROR] Failed: {input_file} -> {error}")
    
    return results


# =============================================================================
# CLI ENTRY POINT
# =============================================================================

__version__ = "2.0.0"

def main():
    """
    CLI entry point for convert_final.py
    
    Usage:
        python convert_final.py input.md [output.pdf]
        python convert_final.py --batch file1.md file2.md --format pdf
        python convert_final.py --config batch-config.json
        python -m tools.pdf.convert_final input.md output.pdf
    """
    import argparse
    import logging
    
    parser = argparse.ArgumentParser(
        description='Convert Markdown to PDF/DOCX/HTML using pipeline architecture',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Convert single file to PDF
  python convert_final.py document.md document.pdf
  
  # Use Playwright renderer with cover page and TOC
  python convert_final.py document.md --renderer playwright --cover --toc
  
  # Batch convert with parallel processing
  python convert_final.py --batch doc1.md doc2.md --threads 4 --format pdf
  
  # Use JSON config for complex batch jobs
  python convert_final.py --config batch-config.json
  
  # Convert to DOCX with glossary
  python convert_final.py doc.md doc.docx --format docx --glossary glossary.yaml
  
  # Validate Markdown before conversion
  python convert_final.py document.md --lint --verbose
  
  # Override metadata from command line
  python convert_final.py doc.md --title "My Report" --author "Jane Doe"

Config File Format (JSON):
  {
    "files": [
      {"input": "doc1.md", "output": "doc1.pdf", "profile": "tech-whitepaper"},
      {"input": "doc2.md", "format": "docx"}
    ],
    "profile": "default",
    "renderer": "playwright",
    "threads": 4
  }
        """
    )
    
    # Input/output arguments
    parser.add_argument('input', nargs='?', help='Input Markdown file')
    parser.add_argument('output', nargs='?', help='Output file (auto-detected if omitted)')
    
    # Batch mode
    parser.add_argument('--batch', nargs='+', metavar='FILE', help='Batch convert multiple files')
    parser.add_argument('--config', help='JSON config file for batch conversion')
    parser.add_argument('--format', default='pdf', choices=['pdf', 'docx', 'html'],
                       help='Output format (default: pdf)')
    parser.add_argument('--output-dir', help='Output directory for all generated files')
    parser.add_argument('--threads', type=int, default=1, 
                       help='Parallel threads for batch processing (default: 1)')
    
    # PDF options
    parser.add_argument('--renderer', default='weasyprint', choices=['weasyprint', 'playwright'],
                       help='PDF renderer (default: weasyprint)')
    parser.add_argument('--cover', '--generate-cover', action='store_true',
                       help='Generate cover page')
    parser.add_argument('--toc', '--generate-toc', action='store_true',
                       help='Generate table of contents')
    parser.add_argument('--watermark', help='Add watermark text')
    
    # Styling options
    parser.add_argument('--css', help='Custom CSS file')
    parser.add_argument('--logo', help='Logo image path')
    parser.add_argument('--profile', help='Style profile name')
    parser.add_argument('--highlight', default='pygments',
                       help='Code highlighting style (default: pygments)')
    parser.add_argument('--reference-docx', help='Reference DOCX template (for DOCX output)')
    
    # Processing options
    parser.add_argument('--glossary', help='Glossary YAML file')
    parser.add_argument('--theme', help='Mermaid theme config JSON')
    parser.add_argument('--crossref-config', help='Pandoc crossref config YAML')
    parser.add_argument('--no-cache', action='store_true', help='Disable diagram caching')
    
    # Metadata overrides
    parser.add_argument('--title', help='Document title (overrides frontmatter)')
    parser.add_argument('--author', help='Author name (overrides frontmatter)')
    parser.add_argument('--organization', '--org', help='Organization name')
    parser.add_argument('--date', help='Document date')
    parser.add_argument('--doc-version', help='Document version')
    parser.add_argument('--classification', help='Classification level')
    parser.add_argument('--doc-type', help='Document type')
    
    # Validation and utilities
    parser.add_argument('--check', action='store_true', help='Check dependencies and exit')
    parser.add_argument('--lint', action='store_true', help='Validate Markdown before conversion')
    parser.add_argument('--log', help='Log file path for CI/automation')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')
    
    args = parser.parse_args()
    
    # Setup logging if requested
    if args.log:
        logging.basicConfig(
            level=logging.DEBUG if args.verbose else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler(), logging.FileHandler(args.log)]
        )
    
    # Check dependencies
    if args.check:
        sys.exit(0 if check_dependencies() else 1)
    
    # Build custom metadata from CLI args
    custom_metadata = {}
    if args.title:
        custom_metadata['title'] = args.title
    if args.author:
        custom_metadata['author'] = args.author
    if args.organization:
        custom_metadata['organization'] = args.organization
    if args.date:
        custom_metadata['date'] = args.date
    if args.doc_version:
        custom_metadata['version'] = args.doc_version
    if args.classification:
        custom_metadata['classification'] = args.classification
    if args.doc_type:
        custom_metadata['type'] = args.doc_type
    
    # Build common kwargs
    def build_kwargs(item_overrides=None):
        kwargs = {
            'verbose': args.verbose,
            'use_cache': not args.no_cache,
            'highlight_style': args.highlight,
            'glossary_file': args.glossary,
            'theme_config': args.theme,
            'css_file': args.css,
            'logo_path': args.logo,
            'profile': args.profile,
            'crossref_config': args.crossref_config,
            'custom_metadata': custom_metadata if custom_metadata else None
        }
        if args.format == 'pdf':
            kwargs.update({
                'renderer': args.renderer,
                'generate_cover': args.cover,
                'generate_toc': args.toc,
                'watermark': args.watermark
            })
        if args.format == 'docx' and args.reference_docx:
            kwargs['reference_docx'] = args.reference_docx
        
        # Apply item-level overrides
        if item_overrides:
            kwargs.update(item_overrides)
        
        return kwargs
    
    # Config file mode
    if args.config:
        config = load_config(args.config)
        files = config.get('files', [])
        threads = args.threads or config.get('threads', 1)
        output_dir = args.output_dir or config.get('output_dir')
        
        # Build file tasks
        file_tasks = []
        for item in files:
            if isinstance(item, dict):
                input_file = item['input']
                output_format = item.get('format', args.format)
                output_file = item.get('output', str(Path(input_file).with_suffix(f'.{output_format}')))
            else:
                input_file = item
                output_format = args.format
                output_file = str(Path(input_file).with_suffix(f'.{output_format}'))
            
            # Resolve output path
            output_file = resolve_output_path(output_file, output_dir)
            
            # Validate if requested
            if args.lint:
                is_valid, issues = validate_markdown(input_file, args.verbose)
                if not is_valid:
                    print(f"[ERROR] Validation failed for {input_file}:")
                    for issue in issues:
                        print(f"  - {issue}")
                    continue
            
            # Build kwargs with item overrides
            item_kwargs = build_kwargs(item if isinstance(item, dict) else None)
            file_tasks.append((input_file, output_file, output_format, item_kwargs))
        
        # Process with threading if requested
        if threads > 1 and len(file_tasks) > 1:
            results = parallel_batch_convert(file_tasks, threads, args.verbose)
        else:
            results = {}
            for input_file, output_file, output_format, kwargs in file_tasks:
                try:
                    format_map = {'pdf': markdown_to_pdf, 'docx': markdown_to_docx, 'html': markdown_to_html}
                    success = format_map[output_format](input_file, output_file, **kwargs)
                    results[input_file] = success
                    if success:
                        print(f"[OK] Generated: {output_file}")
                except Exception as e:
                    results[input_file] = False
                    print(f"[ERROR] Failed: {input_file} -> {e}")
        
        failed = sum(1 for v in results.values() if not v)
        if failed:
            print(f"\n[ERROR] {failed} conversion(s) failed.")
        sys.exit(0 if all(results.values()) else 1)
    
    # Batch mode
    if args.batch:
        file_tasks = []
        for input_file in args.batch:
            if not Path(input_file).exists():
                print(f"[ERROR] Not found: {input_file}")
                continue
            
            # Validate if requested
            if args.lint:
                is_valid, issues = validate_markdown(input_file, args.verbose)
                if not is_valid:
                    print(f"[ERROR] Validation failed for {input_file}:")
                    for issue in issues:
                        print(f"  - {issue}")
                    continue
                elif issues and args.verbose:
                    for issue in issues:
                        print(f"[WARN] {input_file}: {issue}")
            
            output_file = str(Path(input_file).with_suffix(f'.{args.format}'))
            output_file = resolve_output_path(output_file, args.output_dir)
            
            kwargs = build_kwargs()
            file_tasks.append((input_file, output_file, args.format, kwargs))
        
        if not file_tasks:
            print("[ERROR] No valid files to process.")
            sys.exit(1)
        
        # Process with threading if requested
        if args.threads > 1 and len(file_tasks) > 1:
            results = parallel_batch_convert(file_tasks, args.threads, args.verbose)
        else:
            results = {}
            for input_file, output_file, output_format, kwargs in file_tasks:
                try:
                    format_map = {'pdf': markdown_to_pdf, 'docx': markdown_to_docx, 'html': markdown_to_html}
                    success = format_map[output_format](input_file, output_file, **kwargs)
                    results[input_file] = success
                    if success:
                        print(f"[OK] Generated: {output_file}")
                except Exception as e:
                    results[input_file] = False
                    print(f"[ERROR] Failed: {input_file} -> {e}")
        
        failed = sum(1 for v in results.values() if not v)
        if failed:
            print(f"\n[ERROR] {failed} conversion(s) failed.")
        sys.exit(0 if all(results.values()) else 1)
    
    # Single file mode
    if not args.input:
        parser.print_help()
        print("\n[INFO] No input file specified.")
        print("[INFO] Use --batch for multiple files, --config for JSON config")
        sys.exit(0)
    
    # Validate single file
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"[ERROR] Input file not found: {args.input}")
        sys.exit(1)
    
    # Lint if requested
    if args.lint:
        is_valid, issues = validate_markdown(args.input, args.verbose)
        if not is_valid:
            print(f"[ERROR] Validation failed:")
            for issue in issues:
                print(f"  - {issue}")
            sys.exit(1)
        elif issues and args.verbose:
            for issue in issues:
                print(f"[WARN] {issue}")
    
    # Determine output file
    if args.output:
        output_file = args.output
    else:
        output_file = str(input_path.with_suffix(f'.{args.format}'))
    
    output_file = resolve_output_path(output_file, args.output_dir)
    
    # Show conversion info
    print(f"\nConverting {args.input} to {output_file}...")
    print(f"  Renderer: {args.renderer}")
    if args.cover:
        print(f"  Cover page: enabled")
    if args.toc:
        print(f"  Table of contents: enabled")
    if args.profile:
        print(f"  Profile: {args.profile}")
    
    # Build kwargs and convert
    kwargs = build_kwargs()
    
    try:
        if args.format == 'pdf':
            success = markdown_to_pdf(args.input, output_file, **kwargs)
        elif args.format == 'docx':
            success = markdown_to_docx(args.input, output_file, **kwargs)
        else:  # html
            success = markdown_to_html(args.input, output_file, **kwargs)
        
        if success:
            print(f"[OK] Created: {output_file}")
        sys.exit(0 if success else 1)
        
    except Exception as e:
        print(f"[ERROR] Conversion failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
